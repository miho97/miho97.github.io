---
layout: post
title: "Fisher Information Matrix"
tags: [optimization, math, ai]
published: true
permalink: /posts/fisher/
---


Say we want to measure suprise of the event occuring. We assume we have the definition of underlying probability space $(\Omega, \mathcal{F}, \Bbb{P}).$ 
That is, we have exact probabilities of every element of $\mathcal{F}.$
Increase of the probability of the event occuring should be inversly proportional to suprise of the event occuring. So for $A\in\mathcal{F}$ we need inverse relation between $s(A)$ and $\Bbb{P}(A)$.  What if we define $s(A) = \frac{1}{\Bbb{P}(A)}.$ Naturally events with zero probability are a problem. Our usage of suprise function can be restricted on support set but instead we can transform the relations to keep the inverse property. Let us define $s(A) = \frac{1}{\log(\Bbb{P}(A))}$
Suprise of two independent events should be equal to sum of suprises of respective parts. Additivity on independent event is expected.
If we define $s : \mathcal{F} \rightarrow \Bbb{R}$, for $A,B\in\mathcal{F}, A \cap B = \emptyset$, we want: $s(A\cup B) = s(A)+s(B).$
Inductively we continue for arbitrary number of elements.